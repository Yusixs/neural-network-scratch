{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8025a3cb-5d4c-4aea-8ace-38af5e19b9f3",
      "metadata": {
        "id": "8025a3cb-5d4c-4aea-8ace-38af5e19b9f3",
        "outputId": "44ca2ce9-5718-4ef7-fc54-3623299057a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hoxto\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad52767-3b93-44e2-944d-0d89bc4c373b",
      "metadata": {
        "id": "2ad52767-3b93-44e2-944d-0d89bc4c373b"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, Y, split = 0.25):\n",
        "\n",
        "  length = X.shape[0]\n",
        "  index_split = int(length * split)\n",
        "\n",
        "  # Generates random order of numbers from 0 till length\n",
        "  indices = np.random.permutation(length)\n",
        "  test_indices, train_indices = indices[:index_split], indices[index_split:]\n",
        "\n",
        "  X_train, X_test = X[train_indices], X[test_indices]\n",
        "  Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
        "  return X_train, X_test, Y_train, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b22a66f-7ed7-498f-b5ba-9ff0105c42b5",
      "metadata": {
        "id": "3b22a66f-7ed7-498f-b5ba-9ff0105c42b5"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(X, Y):\n",
        "  return (X == Y).sum() / len(Y) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4ccba1-8b4d-4b45-b282-c798f4df2ffa",
      "metadata": {
        "id": "cd4ccba1-8b4d-4b45-b282-c798f4df2ffa"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, X, Y, layer_count, neurons_list):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.lr = None\n",
        "        neurons = [X.shape[1]] + neurons_list + [Y.shape[0]]\n",
        "        for i in range(layer_count + 1):\n",
        "            weight_layer = np.random.random(size=(neurons[i + 1], neurons[i]))\n",
        "            bias_layer = np.random.random(size=(1, neurons[i + 1]))\n",
        "            self.weights.append(weight_layer)\n",
        "            self.biases.append(bias_layer)\n",
        "\n",
        "    # Returns max(0, value) and its derrivative (value basically replaced by 1)\n",
        "    def __relu(self, X):\n",
        "        result = np.maximum(X, 0)\n",
        "        derr = np.where(X > 0, 1, 0)\n",
        "        return result, derr\n",
        "\n",
        "    def __svm_loss_derrivative(self, X, y):\n",
        "        derr = X - X[np.arange(y.shape[0]), y].reshape(-1,1)         # (135,5) - (135, 1) vectorized subtraction\n",
        "        derr[derr < 0] = 0                                           # Creating mask and replacing negative values with 0\n",
        "        loss = np.sum(derr)                                          # Sum up the matrix to compute loss\n",
        "        derr[derr > 0] = 1                                           # Creating mask and replacing positive values with 1\n",
        "        derr[np.arange(y.shape[0]), y] = np.sum(derr, axis = 1) * -1 # Replace target label value with -1 * (sum of 1's in row)\n",
        "        return loss, derr\n",
        "\n",
        "    # Predict values given a feature vector\n",
        "    def forward_propagation(self, prev_features):\n",
        "        cache = []\n",
        "        for weight, bias in zip(self.weights, self.biases):\n",
        "            normal_out = np.dot(prev_features, weight.T) + bias            # Calculate normal output\n",
        "            ldx, ldw, ldb = weight, prev_features, np.ones_like(bias)      # Calculate local derrivatives\n",
        "            relu_out, relu_derr = self.__relu(normal_out)                  # Calculate relu\n",
        "            cache.append((ldx, ldw, ldb, normal_out, relu_out, relu_derr)) # Add to cache\n",
        "            prev_features = relu_out                                       # Set relu output as previous features (for the next layer)\n",
        "        return normal_out, cache                                           # Return non-relu final output\n",
        "\n",
        "    def backward_propagation(self, Y, ud, cache):\n",
        "        gradients = []\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            ldx, ldw, ldb, normal_out, relu_out, relu_derr = cache[i]    # Extract cache values\n",
        "            dx = np.dot(ud * relu_derr, ldx)                             # Calculate dx\n",
        "            dw = np.dot(ldw.T, ud) / Y.shape[0]                          # Calculate dw (and normalize)\n",
        "            db = np.sum(ud, axis=0) / Y.shape[0]                         # Calculate db (and normalize)\n",
        "            ud = dx                                                      # dx becomes upper derrivative for previous layer\n",
        "            gradients.insert(0, (dw, db))                                # Add gradients to list\n",
        "        return gradients\n",
        "\n",
        "    # Applying gradeint descent on weights and biases\n",
        "    def __gradient_descent(self, gradients):\n",
        "        for i in range(len(self.weights)):\n",
        "            dw, db = gradients[i]\n",
        "            self.weights[i] -= self.lr * dw.T\n",
        "            self.biases[i] -= self.lr * db.reshape(1, -1)\n",
        "\n",
        "    # Returns Logits of test data\n",
        "    def Predict_Confidence(self, X):\n",
        "        predictions, cache = self.forward_propagation(X)\n",
        "        return predictions\n",
        "\n",
        "    # Predicts class of test data (argmax)\n",
        "    def Predict_Class(self, X):\n",
        "        X = self.Predict_Confidence(X)\n",
        "        return np.argmax(X, axis=1)\n",
        "\n",
        "    def Train(self, X, Y, alpha = 0.0001, loss_at_iter = 50, max_iter = None):\n",
        "\n",
        "        # Setting up some stuff\n",
        "        convergence_check = False\n",
        "        self.lr = alpha\n",
        "        X = np.array(X) # Dimensions are: (samples, features)\n",
        "        Y = np.array(Y).reshape(-1,) # Dimensions are: (samples, 1)\n",
        "\n",
        "        # Handling Termination by changes in old and new loss values incase max_iter is not defined\n",
        "        if max_iter is None:\n",
        "          convergence_check = True\n",
        "          print(\"Convergence Criteria will be used\")\n",
        "\n",
        "        old_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        while(True):\n",
        "\n",
        "          predictions, cache = self.forward_propagation(X)\n",
        "          loss, derrivative = self.__svm_loss_derrivative(predictions, Y) # Returns loss and derrivative\n",
        "\n",
        "          if (counter % loss_at_iter == 0):\n",
        "              print(f\"Loss at iteration {counter} = {loss}\")\n",
        "\n",
        "          gradients = self.backward_propagation(Y, derrivative, cache)\n",
        "          self.__gradient_descent(gradients)\n",
        "\n",
        "          # Either do convergence check or max iteration check\n",
        "          if convergence_check:\n",
        "            if abs(old_loss - loss) / loss < 0.0001: # If the loss difference is lesser than 0.01%, break\n",
        "              print(\"Convergence Reached\")\n",
        "              break\n",
        "          else:\n",
        "            if (counter >= max_iter - 1): # If max iterations are reached, break\n",
        "              print(\"Maximum Iterations Reached\")\n",
        "              break\n",
        "\n",
        "          old_loss = loss\n",
        "          counter +=1\n",
        "\n",
        "        print(f\"Final Loss at iteration {counter} = {loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415fc809-481d-4015-be1b-569a721f4ee1",
      "metadata": {
        "id": "415fc809-481d-4015-be1b-569a721f4ee1",
        "outputId": "07638587-d913-40e6-defb-accb7aeeb0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convergence Criteria will be used\n",
            "Loss at iteration 0 = 10090.113230649671\n",
            "Loss at iteration 100 = 0.039118162766396125\n",
            "Loss at iteration 200 = 0.03603578697226606\n",
            "Loss at iteration 300 = 0.03475219889201231\n",
            "Loss at iteration 400 = 0.032574120962116204\n",
            "Loss at iteration 500 = 0.030397629992584108\n",
            "Loss at iteration 600 = 0.028299575139761224\n",
            "Loss at iteration 700 = 0.026735201299303846\n",
            "Loss at iteration 800 = 0.03598337402539986\n",
            "Loss at iteration 900 = 0.036757588502976724\n",
            "Loss at iteration 1000 = 0.031770415847623745\n",
            "Loss at iteration 1100 = 0.023317191003888738\n",
            "Loss at iteration 1200 = 0.0536286100168466\n",
            "Loss at iteration 1300 = 0.026956564534642347\n",
            "Loss at iteration 1400 = 0.10135095469315747\n",
            "Loss at iteration 1500 = 0.024999709619085486\n",
            "Loss at iteration 1600 = 0.0617588063887462\n",
            "Loss at iteration 1700 = 0.03780824845969466\n",
            "Loss at iteration 1800 = 0.09895731785349504\n",
            "Loss at iteration 1900 = 0.02211799816796134\n",
            "Loss at iteration 2000 = 0.0364930577969238\n",
            "Loss at iteration 2100 = 0.03302282424723746\n",
            "Loss at iteration 2200 = 0.04756188653172355\n",
            "Loss at iteration 2300 = 0.058715749540533935\n",
            "Loss at iteration 2400 = 0.04393858363107306\n",
            "Loss at iteration 2500 = 0.05673568320741662\n",
            "Loss at iteration 2600 = 0.019299452259791572\n",
            "Loss at iteration 2700 = 0.060513007848652656\n",
            "Loss at iteration 2800 = 0.0567152260941457\n",
            "Loss at iteration 2900 = 0.01960712133796516\n",
            "Loss at iteration 3000 = 0.09290868890997928\n",
            "Loss at iteration 3100 = 0.023632610136052\n",
            "Loss at iteration 3200 = 0.024966674378529063\n",
            "Loss at iteration 3300 = 0.024955479287975812\n",
            "Loss at iteration 3400 = 0.0227944820035626\n",
            "Loss at iteration 3500 = 0.027888116777133654\n",
            "Loss at iteration 3600 = 0.08989070587493142\n",
            "Loss at iteration 3700 = 0.03556793355960952\n",
            "Loss at iteration 3800 = 0.05292220712525797\n",
            "Loss at iteration 3900 = 0.054890709465199805\n",
            "Loss at iteration 4000 = 0.056273760408663165\n",
            "Loss at iteration 4100 = 0.04044765221776547\n",
            "Loss at iteration 4200 = 0.018306304524688954\n",
            "Loss at iteration 4300 = 0.050432319757589994\n",
            "Loss at iteration 4400 = 0.052188911390264714\n",
            "Loss at iteration 4500 = 0.05458970689208442\n",
            "Loss at iteration 4600 = 0.042391184606367815\n",
            "Loss at iteration 4700 = 0.0478195560934056\n",
            "Loss at iteration 4800 = 0.050793491059401674\n",
            "Loss at iteration 4900 = 0.0539649480229798\n",
            "Loss at iteration 5000 = 0.018510226388543494\n",
            "Loss at iteration 5100 = 0.04775391634659165\n",
            "Loss at iteration 5200 = 0.04957075489906604\n",
            "Loss at iteration 5300 = 0.05155814592208241\n",
            "Loss at iteration 5400 = 0.0384679671121857\n",
            "Loss at iteration 5500 = 0.04672020953987577\n",
            "Loss at iteration 5600 = 0.04992430901987799\n",
            "Loss at iteration 5700 = 0.024466209857566312\n",
            "Loss at iteration 5800 = 0.018449447311010836\n",
            "Loss at iteration 5900 = 0.029239533564073383\n",
            "Loss at iteration 6000 = 0.047998425257056\n",
            "Loss at iteration 6100 = 0.020383771952736218\n",
            "Loss at iteration 6200 = 0.028085928980940977\n",
            "Loss at iteration 6300 = 0.05030558723827028\n",
            "Loss at iteration 6400 = 0.01681167944830264\n",
            "Loss at iteration 6500 = 0.02560055465943023\n",
            "Loss at iteration 6600 = 0.019732196370361166\n",
            "Convergence Reached\n",
            "Final Loss at iteration 6656 = 0.016890816099152772\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X, Y, Y_names = iris['data'], iris['target'], iris['target_names']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 0.1)\n",
        "\n",
        "nn = NeuralNetwork(X, Y_names, 3, [5,10,5])\n",
        "nn.Train(X_train, Y_train, 0.01, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ea1bfd-f794-4fcc-87bd-382b6e7dde9b",
      "metadata": {
        "id": "10ea1bfd-f794-4fcc-87bd-382b6e7dde9b",
        "outputId": "502cb958-cac1-4b56-f4ac-875b82d34b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: [0 1 1 1 0 0 2 2 0 2 0 0 1 0 1]\n",
            "Actual: [0 1 1 1 0 0 2 2 0 2 0 0 1 0 1]\n",
            "Accuracy: 100.0\n"
          ]
        }
      ],
      "source": [
        "predicted = nn.Predict_Class(X_test)\n",
        "print(f\"Predicted: {predicted}\")\n",
        "print(f\"Actual: {Y_test}\")\n",
        "print(f\"Accuracy: {calculate_accuracy(predicted, Y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65901b88-13c4-402b-bf5a-93d62b1bb600",
      "metadata": {
        "id": "65901b88-13c4-402b-bf5a-93d62b1bb600"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d9a4f8-cf3f-43c9-9d1f-d9df9d00c318",
      "metadata": {
        "id": "64d9a4f8-cf3f-43c9-9d1f-d9df9d00c318"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}